{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from game import Game, Player, Move\n",
    "import numpy as np\n",
    "from copy import copy, deepcopy\n",
    "from hashlib import sha1\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "\n",
    "class Training_game(Game):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def move(self, move, id):\n",
    "        return super()._Game__move(move[0], move[1], id)\n",
    "\n",
    "    def set_board(self, board):\n",
    "        self._board = board\n",
    "\n",
    "    def change_board(self, move, player_id):\n",
    "        current_board = copy(self._board)\n",
    "        success = self.move(move, player_id)\n",
    "        if success:\n",
    "            return self.get_board(), current_board\n",
    "        else:\n",
    "            return None \n",
    "\n",
    "    def play(self, player1: Player, player2: Player, first = True) -> int:\n",
    "            '''Play the game. Returns the winning player'''\n",
    "            players = [player1, player2]\n",
    "            winner = -1\n",
    "            while winner < 0:\n",
    "                self.current_player_idx += 1\n",
    "                self.current_player_idx %= len(players)\n",
    "                ok = False\n",
    "                while not ok:\n",
    "                    from_pos, slide = players[self.current_player_idx].make_move(\n",
    "                        self)\n",
    "                    ok = self.move([from_pos, slide], self.current_player_idx)\n",
    "                winner = self.check_winner()\n",
    "            p = player1 if first else player2\n",
    "            p.states.append(self._board)\n",
    "            return winner\n",
    "\n",
    "\n",
    "class RandomPlayer(Player):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
    "        from_pos = (random.randint(0, 4), random.randint(0, 4))\n",
    "        move = random.choice([Move.TOP, Move.BOTTOM, Move.LEFT, Move.RIGHT])\n",
    "        return from_pos, move\n",
    "\n",
    "\n",
    "class TrainingAgent(Player):\n",
    "    def __init__(self, player_id, lr, gamma, num_training_games, exploit) -> None:\n",
    "        super().__init__()\n",
    "        self.player_value = player_id\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.exploit = exploit\n",
    "        self.states = []\n",
    "        self.Q = {}\n",
    "        self.games_played = 0\n",
    "        self.num_training_games = num_training_games\n",
    "\n",
    "\n",
    "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
    "        current_board = np.copy(game.get_board())\n",
    "        self.states.append(current_board)\n",
    "        e = np.exp(-1.5 * (self.games_played / self.num_training_games)) \n",
    "        epsilon = 0 if (self.games_played / self.num_training_games >= 1) or self.exploit else e\n",
    "        if (np.random.rand() < epsilon):\n",
    "            moves = self.get_legal_moves(current_board)\n",
    "            index = random.randint(0, len(moves) - 1)\n",
    "            return moves[index]\n",
    "\n",
    "        else:\n",
    "            best_move = None\n",
    "            highest_eval = -np.inf\n",
    "            moves = self.get_legal_moves(game.get_board())\n",
    "            for move in moves:\n",
    "                current_board = np.copy(game.get_board())\n",
    "                new_game = Training_game()\n",
    "                new_game.set_board(current_board)\n",
    "                new_board, prev_pos = new_game.change_board(move, self.player_value)\n",
    "                hashed_board_state = self.hash_board(new_board)\n",
    "                table_result = self.Q.get(hashed_board_state)\n",
    "                eval = 0 if table_result == None else table_result\n",
    "                if (eval > highest_eval):\n",
    "                    highest_eval = eval\n",
    "                    best_move = move\n",
    "            return best_move\n",
    "                    \n",
    "                \n",
    "    def train(self, num_games, opponent=None):\n",
    "            r = opponent if opponent != None else RandomPlayer()\n",
    "            self.num_games_played = 0\n",
    "            for _ in tqdm(range(num_games)):\n",
    "                t = Training_game()\n",
    "        \n",
    "                result = t.play(r, self, first=False)\n",
    "\n",
    "                reward = 1 if self.player_value == result else -1\n",
    "\n",
    "                for state in reversed(self.states):\n",
    "                    state_to_eval = state\n",
    "                    hashed_state = self.hash_board(state_to_eval)\n",
    "                    if (self.Q.get(hashed_state) == None):\n",
    "                            self.Q[hashed_state] = 0\n",
    "                    self.Q[hashed_state] += self.lr * (self.gamma * reward - self.Q[hashed_state])\n",
    "                    for _ in range(4):\n",
    "                        if (self.Q.get(hashed_state) == None):\n",
    "                            self.Q[hashed_state] = 0\n",
    "                        self.Q[hashed_state] += self.lr * (self.gamma * reward - self.Q[hashed_state])\n",
    "                        state_to_eval = np.rot90(state_to_eval)\n",
    "                    reward = self.Q[hashed_state] \n",
    "                self.states.clear()\n",
    "                self.games_played += 1\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "    def get_legal_moves(self, board: np.ndarray):\n",
    "        moves = list()\n",
    "        rows, cols = board.shape\n",
    "        for i in range(rows):\n",
    "            if (board[i, 0] == -1):\n",
    "                pickable_piece = (0,i)\n",
    "                if (i == 0):\n",
    "                    insertions = (Move.BOTTOM, Move.RIGHT)\n",
    "                elif (i == 4):\n",
    "                    insertions = (Move.TOP, Move.RIGHT)\n",
    "                else:\n",
    "                    insertions = (Move.TOP, Move.BOTTOM, Move.RIGHT)\n",
    "                \n",
    "                for insertion in insertions:\n",
    "                    moves.append((pickable_piece, insertion))\n",
    "            \n",
    "            if (board[i, 4] == -1):\n",
    "                pickable_piece = (4,i)\n",
    "                if (i == 0):\n",
    "                    insertions = (Move.BOTTOM, Move.LEFT)\n",
    "                elif (i == 4):\n",
    "                    insertions = [Move.TOP, Move.LEFT]\n",
    "                else:\n",
    "                    insertions = (Move.TOP, Move.BOTTOM, Move.LEFT)\n",
    "                for insertion in insertions:\n",
    "                    moves.append((pickable_piece, insertion))\n",
    "        for j in range(cols):\n",
    "            if(board[0, j] == self.player_value or board[0, j] == -1):\n",
    "                pickable_piece = (j,0)\n",
    "                if(j == 0):\n",
    "                    insertions = (Move.BOTTOM, Move.RIGHT)\n",
    "                elif (j == 4):\n",
    "                    insertions = (Move.BOTTOM, Move.LEFT)\n",
    "                else:\n",
    "                    insertions = (Move.LEFT, Move.RIGHT, Move.BOTTOM)\n",
    "                for insertion in insertions:\n",
    "                    moves.append((pickable_piece, insertion))\n",
    "            if(board[4, j] == self.player_value or board[4, j] == -1):\n",
    "                pickable_piece = (j,4)\n",
    "                if (j == 0):\n",
    "                    insertions = (Move.TOP, Move.RIGHT)\n",
    "                elif (j == 4):\n",
    "                    insertions = (Move.TOP, Move.LEFT)\n",
    "                else:\n",
    "                    insertions = (Move.LEFT, Move.RIGHT, Move.TOP)\n",
    "                for insertion in insertions:\n",
    "                    moves.append((pickable_piece, insertion))\n",
    "        return moves\n",
    "\n",
    "    def hash_board(self, board):\n",
    "        sha_hash = sha1()\n",
    "        sha_hash.update(board.tobytes())\n",
    "        return sha_hash.hexdigest()\n",
    "\n",
    "    def epsilon(self, n):\n",
    "        xr = 1 - np.log2(1+(n / self.num_training_games))\n",
    "        epsilon = xr if n < self.num_training_games else 0\n",
    "        return epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:10<00:00, 187.74it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 116.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player winrate: 0.399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 202/2000 [00:02<00:19, 92.14it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m w \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mplayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_training_games\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     r \u001b[38;5;241m=\u001b[39m RandomPlayer()\n\u001b[1;32m     14\u001b[0m     num_games \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "Cell \u001b[0;32mIn [1], line 104\u001b[0m, in \u001b[0;36mTrainingAgent.train\u001b[0;34m(self, num_games, opponent)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_games)):\n\u001b[1;32m    102\u001b[0m     t \u001b[38;5;241m=\u001b[39m Training_game()\n\u001b[0;32m--> 104\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayer_value \u001b[38;5;241m==\u001b[39m result \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates):\n",
      "Cell \u001b[0;32mIn [1], line 41\u001b[0m, in \u001b[0;36mTraining_game.play\u001b[0;34m(self, player1, player2, first)\u001b[0m\n\u001b[1;32m     38\u001b[0m         from_pos, slide \u001b[38;5;241m=\u001b[39m players[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_player_idx]\u001b[38;5;241m.\u001b[39mmake_move(\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m     40\u001b[0m         ok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmove([from_pos, slide], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_player_idx)\n\u001b[0;32m---> 41\u001b[0m     winner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_winner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m p \u001b[38;5;241m=\u001b[39m player1 \u001b[38;5;28;01mif\u001b[39;00m first \u001b[38;5;28;01melse\u001b[39;00m player2\n\u001b[1;32m     43\u001b[0m p\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_board)\n",
      "File \u001b[0;32m~/computational intelligence/computational_intelligence/quixo/game.py:64\u001b[0m, in \u001b[0;36mGame.check_winner\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m winner \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_board\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m     63\u001b[0m     \u001b[39m# if a player has completed an entire row\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_board[x, \u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_board[x, :] \u001b[39m==\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_board[x, \u001b[39m0\u001b[39;49m]):\n\u001b[1;32m     65\u001b[0m         \u001b[39m# return winner is this guy\u001b[39;00m\n\u001b[1;32m     66\u001b[0m         winner \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_board[x, \u001b[39m0\u001b[39m]\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m winner \u001b[39m>\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m winner \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_current_player():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# #This cell block was used to train the agent, before writing the Q-table to json. Commented out in order to not run it while importing this notebook\n",
    "\n",
    "\n",
    "# num_training_games = 20000\n",
    "# player_id = 1\n",
    "# num_epochs = 100\n",
    "# player = TrainingAgent(player_id, 0.01, 0.9, num_training_games, False)\n",
    "# x = []\n",
    "# w = []\n",
    "\n",
    "# for i in range(num_epochs):\n",
    "#     player.train(num_training_games)\n",
    "#     r = RandomPlayer()\n",
    "#     num_games = 1000\n",
    "#     results = []\n",
    "#     for _ in tqdm(range(num_games)):\n",
    "#         r2 = RandomPlayer()\n",
    "#         t = Training_game()\n",
    "#         result = t.play(r, player, False)\n",
    "#         results.append(result)\n",
    "\n",
    "#     print(f\"player winrate: {results.count(player_id) / num_games}\")\n",
    "\n",
    "#     w.append(results.count(player_id) / num_games)\n",
    "#     x.append(i * num_training_games)\n",
    "    \n",
    "# plt.plot(x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell block that writes the values of Q table to json file. Commented out in order to not overwrite the current state\n",
    "\n",
    "\n",
    "# file_path = 'Q2_000_000_second.json'\n",
    "# with open(file_path, 'w') as file:\n",
    "#     json.dump(player.Q, file, indent=2) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
